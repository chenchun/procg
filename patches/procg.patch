diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index eb65d29..d143fa9 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -47,6 +47,14 @@ enum mem_cgroup_stat_index {
 	MEM_CGROUP_STAT_NSTATS,
 };
 
+enum mem_cgroup_events_index {
+	MEM_CGROUP_EVENTS_PGPGIN,	/* # of pages paged in */
+	MEM_CGROUP_EVENTS_PGPGOUT,	/* # of pages paged out */
+	MEM_CGROUP_EVENTS_PGFAULT,	/* # of page-faults */
+	MEM_CGROUP_EVENTS_PGMAJFAULT,	/* # of major page-faults */
+	MEM_CGROUP_EVENTS_NSTATS,
+};
+
 struct mem_cgroup_reclaim_cookie {
 	struct zone *zone;
 	int priority;
@@ -54,6 +62,9 @@ struct mem_cgroup_reclaim_cookie {
 };
 
 #ifdef CONFIG_MEMCG
+extern const char * const mem_cgroup_stat_names[];
+extern const char * const mem_cgroup_events_names[];
+extern const char * const mem_cgroup_lru_names[];
 /*
  * All "charge" functions with gfp_mask should use GFP_KERNEL or
  * (gfp_mask & GFP_RECLAIM_MASK). In current implementatin, memcg doesn't
@@ -87,6 +98,7 @@ extern void mem_cgroup_uncharge_end(void);
 extern void mem_cgroup_uncharge_page(struct page *page);
 extern void mem_cgroup_uncharge_cache_page(struct page *page);
 
+extern bool mem_cgroup_is_root(struct mem_cgroup *memcg);
 bool __mem_cgroup_same_or_subtree(const struct mem_cgroup *root_memcg,
 				  struct mem_cgroup *memcg);
 bool task_in_mem_cgroup(struct task_struct *task,
@@ -98,6 +110,13 @@ extern struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);
 extern struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg);
 extern struct mem_cgroup *mem_cgroup_from_css(struct cgroup_subsys_state *css);
 
+extern u64 mem_cgroup_usage(struct mem_cgroup *memcg, bool swap);
+extern u64 mem_cgroup_get_limit(struct mem_cgroup *memcg, bool swap);
+extern long mem_cgroup_read_stat(struct mem_cgroup *memcg, enum mem_cgroup_stat_index idx);
+extern unsigned long mem_cgroup_read_events(struct mem_cgroup *memcg, enum mem_cgroup_events_index idx);
+extern unsigned long mem_cgroup_nr_lru_pages(struct mem_cgroup *memcg,
+			unsigned int lru_mask);
+
 static inline
 bool mm_match_cgroup(const struct mm_struct *mm, const struct mem_cgroup *memcg)
 {
@@ -299,6 +318,11 @@ static inline bool mm_match_cgroup(struct mm_struct *mm,
 	return true;
 }
 
+static inline bool mem_cgroup_is_root(const struct mem_cgroup *memcg)
+{
+    return true;
+}
+
 static inline bool task_in_mem_cgroup(struct task_struct *task,
 				      const struct mem_cgroup *memcg)
 {
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 2f3c21c..1bca501 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -88,7 +88,7 @@ static int really_do_swap_account __initdata;
 #endif
 
 
-static const char * const mem_cgroup_stat_names[] = {
+const char * const mem_cgroup_stat_names[] = {
 	"cache",
 	"rss",
 	"rss_huge",
@@ -96,29 +96,24 @@ static const char * const mem_cgroup_stat_names[] = {
 	"writeback",
 	"swap",
 };
+EXPORT_SYMBOL(mem_cgroup_stat_names);
 
-enum mem_cgroup_events_index {
-	MEM_CGROUP_EVENTS_PGPGIN,	/* # of pages paged in */
-	MEM_CGROUP_EVENTS_PGPGOUT,	/* # of pages paged out */
-	MEM_CGROUP_EVENTS_PGFAULT,	/* # of page-faults */
-	MEM_CGROUP_EVENTS_PGMAJFAULT,	/* # of major page-faults */
-	MEM_CGROUP_EVENTS_NSTATS,
-};
-
-static const char * const mem_cgroup_events_names[] = {
+const char * const mem_cgroup_events_names[] = {
 	"pgpgin",
 	"pgpgout",
 	"pgfault",
 	"pgmajfault",
 };
+EXPORT_SYMBOL(mem_cgroup_events_names);
 
-static const char * const mem_cgroup_lru_names[] = {
+const char * const mem_cgroup_lru_names[] = {
 	"inactive_anon",
 	"active_anon",
 	"inactive_file",
 	"active_file",
 	"unevictable",
 };
+EXPORT_SYMBOL(mem_cgroup_lru_names);
 
 /*
  * Per memcg event counter is incremented at every pagein/pageout. With THP,
@@ -516,11 +511,17 @@ struct cgroup_subsys_state *vmpressure_to_css(struct vmpressure *vmpr)
 	return &container_of(vmpr, struct mem_cgroup, vmpressure)->css;
 }
 
-static inline bool mem_cgroup_is_root(struct mem_cgroup *memcg)
+static inline bool __mem_cgroup_is_root(struct mem_cgroup *memcg)
 {
 	return (memcg == root_mem_cgroup);
 }
 
+bool mem_cgroup_is_root(struct mem_cgroup *memcg)
+{
+    return __mem_cgroup_is_root(memcg);
+}
+EXPORT_SYMBOL(mem_cgroup_is_root);
+
 /*
  * We restrict the id in the range of [1, 65535], so it can fit into
  * an unsigned short.
@@ -560,7 +561,7 @@ void sock_update_memcg(struct sock *sk)
 		 * decision in this case.
 		 */
 		if (sk->sk_cgrp) {
-			BUG_ON(mem_cgroup_is_root(sk->sk_cgrp->memcg));
+			BUG_ON(__mem_cgroup_is_root(sk->sk_cgrp->memcg));
 			css_get(&sk->sk_cgrp->memcg->css);
 			return;
 		}
@@ -568,7 +569,7 @@ void sock_update_memcg(struct sock *sk)
 		rcu_read_lock();
 		memcg = mem_cgroup_from_task(current);
 		cg_proto = sk->sk_prot->proto_cgroup(memcg);
-		if (!mem_cgroup_is_root(memcg) &&
+		if (!__mem_cgroup_is_root(memcg) &&
 		    memcg_proto_active(cg_proto) &&
 		    css_tryget_online(&memcg->css)) {
 			sk->sk_cgrp = cg_proto;
@@ -590,7 +591,7 @@ void sock_release_memcg(struct sock *sk)
 
 struct cg_proto *tcp_proto_cgroup(struct mem_cgroup *memcg)
 {
-	if (!memcg || mem_cgroup_is_root(memcg))
+	if (!memcg || __mem_cgroup_is_root(memcg))
 		return NULL;
 
 	return &memcg->tcp_mem;
@@ -867,7 +868,7 @@ mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)
  * common workload, threashold and synchonization as vmstat[] should be
  * implemented.
  */
-static long mem_cgroup_read_stat(struct mem_cgroup *memcg,
+long mem_cgroup_read_stat(struct mem_cgroup *memcg,
 				 enum mem_cgroup_stat_index idx)
 {
 	long val = 0;
@@ -884,6 +885,7 @@ static long mem_cgroup_read_stat(struct mem_cgroup *memcg,
 	put_online_cpus();
 	return val;
 }
+EXPORT_SYMBOL(mem_cgroup_read_stat);
 
 static void mem_cgroup_swap_statistics(struct mem_cgroup *memcg,
 					 bool charge)
@@ -892,7 +894,7 @@ static void mem_cgroup_swap_statistics(struct mem_cgroup *memcg,
 	this_cpu_add(memcg->stat->count[MEM_CGROUP_STAT_SWAP], val);
 }
 
-static unsigned long mem_cgroup_read_events(struct mem_cgroup *memcg,
+unsigned long mem_cgroup_read_events(struct mem_cgroup *memcg,
 					    enum mem_cgroup_events_index idx)
 {
 	unsigned long val = 0;
@@ -909,6 +911,7 @@ static unsigned long mem_cgroup_read_events(struct mem_cgroup *memcg,
 	put_online_cpus();
 	return val;
 }
+EXPORT_SYMBOL(mem_cgroup_read_events);
 
 static void mem_cgroup_charge_statistics(struct mem_cgroup *memcg,
 					 struct page *page,
@@ -971,7 +974,7 @@ static unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,
 	return nr;
 }
 
-static unsigned long mem_cgroup_nr_lru_pages(struct mem_cgroup *memcg,
+unsigned long mem_cgroup_nr_lru_pages(struct mem_cgroup *memcg,
 			unsigned int lru_mask)
 {
 	unsigned long nr = 0;
@@ -981,6 +984,7 @@ static unsigned long mem_cgroup_nr_lru_pages(struct mem_cgroup *memcg,
 		nr += mem_cgroup_node_nr_lru_pages(memcg, nid, lru_mask);
 	return nr;
 }
+EXPORT_SYMBOL(mem_cgroup_nr_lru_pages);
 
 static bool mem_cgroup_event_ratelimit(struct mem_cgroup *memcg,
 				       enum mem_cgroup_events_target target)
@@ -1054,6 +1058,7 @@ struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)
 
 	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));
 }
+EXPORT_SYMBOL(mem_cgroup_from_task);
 
 static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)
 {
@@ -1731,7 +1736,7 @@ static int mem_cgroup_count_children(struct mem_cgroup *memcg)
 /*
  * Return the memory (and swap, if configured) limit for a memcg.
  */
-static u64 mem_cgroup_get_limit(struct mem_cgroup *memcg)
+u64 mem_cgroup_get_limit(struct mem_cgroup *memcg, bool swap)
 {
 	u64 limit;
 
@@ -1740,7 +1745,7 @@ static u64 mem_cgroup_get_limit(struct mem_cgroup *memcg)
 	/*
 	 * Do not consider swap space if we cannot swap due to swappiness
 	 */
-	if (mem_cgroup_swappiness(memcg)) {
+	if (swap && mem_cgroup_swappiness(memcg)) {
 		u64 memsw;
 
 		limit += total_swap_pages << PAGE_SHIFT;
@@ -1755,6 +1760,7 @@ static u64 mem_cgroup_get_limit(struct mem_cgroup *memcg)
 
 	return limit;
 }
+EXPORT_SYMBOL(mem_cgroup_get_limit);
 
 static void mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,
 				     int order)
@@ -1776,7 +1782,7 @@ static void mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,
 	}
 
 	check_panic_on_oom(CONSTRAINT_MEMCG, gfp_mask, order, NULL);
-	totalpages = mem_cgroup_get_limit(memcg) >> PAGE_SHIFT ? : 1;
+	totalpages = mem_cgroup_get_limit(memcg, true) >> PAGE_SHIFT ? : 1;
 	for_each_mem_cgroup_tree(iter, memcg) {
 		struct css_task_iter it;
 		struct task_struct *task;
@@ -2657,7 +2663,7 @@ static int mem_cgroup_try_charge(struct mem_cgroup *memcg,
 	int nr_oom_retries = MEM_CGROUP_RECLAIM_RETRIES;
 	int ret;
 
-	if (mem_cgroup_is_root(memcg))
+	if (__mem_cgroup_is_root(memcg))
 		goto done;
 	/*
 	 * Unlike in global OOM situations, memcg is not in a physical
@@ -2752,7 +2758,7 @@ static struct mem_cgroup *mem_cgroup_try_charge_mm(struct mm_struct *mm,
 static void __mem_cgroup_cancel_charge(struct mem_cgroup *memcg,
 				       unsigned int nr_pages)
 {
-	if (!mem_cgroup_is_root(memcg)) {
+	if (!__mem_cgroup_is_root(memcg)) {
 		unsigned long bytes = nr_pages * PAGE_SIZE;
 
 		res_counter_uncharge(&memcg->res, bytes);
@@ -2770,7 +2776,7 @@ static void __mem_cgroup_cancel_local_charge(struct mem_cgroup *memcg,
 {
 	unsigned long bytes = nr_pages * PAGE_SIZE;
 
-	if (mem_cgroup_is_root(memcg))
+	if (__mem_cgroup_is_root(memcg))
 		return;
 
 	res_counter_uncharge_until(&memcg->res, memcg->res.parent, bytes);
@@ -2905,7 +2911,7 @@ static DEFINE_MUTEX(activate_kmem_mutex);
 
 static inline bool memcg_can_account_kmem(struct mem_cgroup *memcg)
 {
-	return !mem_cgroup_disabled() && !mem_cgroup_is_root(memcg) &&
+	return !mem_cgroup_disabled() && !__mem_cgroup_is_root(memcg) &&
 		memcg_kmem_is_active(memcg);
 }
 
@@ -3470,7 +3476,7 @@ void __memcg_kmem_commit_charge(struct page *page, struct mem_cgroup *memcg,
 {
 	struct page_cgroup *pc;
 
-	VM_BUG_ON(mem_cgroup_is_root(memcg));
+	VM_BUG_ON(__mem_cgroup_is_root(memcg));
 
 	/* The page allocation failed. Revert */
 	if (!page) {
@@ -3513,7 +3519,7 @@ void __memcg_kmem_uncharge_pages(struct page *page, int order)
 	if (!memcg)
 		return;
 
-	VM_BUG_ON_PAGE(mem_cgroup_is_root(memcg), page);
+	VM_BUG_ON_PAGE(__mem_cgroup_is_root(memcg), page);
 	memcg_uncharge_kmem(memcg, PAGE_SIZE << order);
 }
 #else
@@ -3660,7 +3666,7 @@ static int mem_cgroup_move_parent(struct page *page,
 	unsigned long uninitialized_var(flags);
 	int ret;
 
-	VM_BUG_ON(mem_cgroup_is_root(child));
+	VM_BUG_ON(__mem_cgroup_is_root(child));
 
 	ret = -EBUSY;
 	if (!get_page_unless_zero(page))
@@ -4007,7 +4013,7 @@ __mem_cgroup_uncharge_common(struct page *page, enum charge_type ctype,
 	 * replacement page, so leave it alone when phasing out the
 	 * page that is unused after the migration.
 	 */
-	if (!end_migration && !mem_cgroup_is_root(memcg))
+	if (!end_migration && !__mem_cgroup_is_root(memcg))
 		mem_cgroup_do_uncharge(memcg, nr_pages, ctype);
 
 	return memcg;
@@ -4140,7 +4146,7 @@ void mem_cgroup_uncharge_swap(swp_entry_t ent)
 		 * We uncharge this because swap is freed.  This memcg can
 		 * be obsolete one. We avoid calling css_tryget_online().
 		 */
-		if (!mem_cgroup_is_root(memcg))
+		if (!__mem_cgroup_is_root(memcg))
 			res_counter_uncharge(&memcg->memsw, PAGE_SIZE);
 		mem_cgroup_swap_statistics(memcg, false);
 		css_put(&memcg->css);
@@ -4785,7 +4791,7 @@ static ssize_t mem_cgroup_force_empty_write(struct kernfs_open_file *of,
 {
 	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
 
-	if (mem_cgroup_is_root(memcg))
+	if (__mem_cgroup_is_root(memcg))
 		return -EINVAL;
 	return mem_cgroup_force_empty(memcg) ?: nbytes;
 }
@@ -4847,11 +4853,11 @@ static unsigned long mem_cgroup_recursive_stat(struct mem_cgroup *memcg,
 	return val;
 }
 
-static inline u64 mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)
+u64 mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)
 {
 	u64 val;
 
-	if (!mem_cgroup_is_root(memcg)) {
+	if (!__mem_cgroup_is_root(memcg)) {
 		if (!swap)
 			return res_counter_read_u64(&memcg->res, RES_USAGE);
 		else
@@ -4870,6 +4876,7 @@ static inline u64 mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)
 
 	return val << PAGE_SHIFT;
 }
+EXPORT_SYMBOL(mem_cgroup_usage);
 
 static u64 mem_cgroup_read_u64(struct cgroup_subsys_state *css,
 				   struct cftype *cft)
@@ -5053,7 +5060,7 @@ static ssize_t mem_cgroup_write(struct kernfs_open_file *of,
 
 	switch (name) {
 	case RES_LIMIT:
-		if (mem_cgroup_is_root(memcg)) { /* Can't set limit on root */
+		if (__mem_cgroup_is_root(memcg)) { /* Can't set limit on root */
 			ret = -EINVAL;
 			break;
 		}
@@ -6446,7 +6453,7 @@ static int mem_cgroup_do_precharge(unsigned long count)
 	int batch_count = PRECHARGE_COUNT_AT_ONCE;
 	struct mem_cgroup *memcg = mc.to;
 
-	if (mem_cgroup_is_root(memcg)) {
+	if (__mem_cgroup_is_root(memcg)) {
 		mc.precharge += count;
 		/* we don't need css_get for root */
 		return ret;
@@ -6764,14 +6771,14 @@ static void __mem_cgroup_clear_mc(void)
 	/* we must fixup refcnts and charges */
 	if (mc.moved_swap) {
 		/* uncharge swap account from the old cgroup */
-		if (!mem_cgroup_is_root(mc.from))
+		if (!__mem_cgroup_is_root(mc.from))
 			res_counter_uncharge(&mc.from->memsw,
 						PAGE_SIZE * mc.moved_swap);
 
 		for (i = 0; i < mc.moved_swap; i++)
 			css_put(&mc.from->css);
 
-		if (!mem_cgroup_is_root(mc.to)) {
+		if (!__mem_cgroup_is_root(mc.to)) {
 			/*
 			 * we charged both to->res and to->memsw, so we should
 			 * uncharge to->res.
